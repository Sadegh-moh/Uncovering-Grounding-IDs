<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="icon" type="image/png" href="assets/icon.jpg" />
  <title>Uncovering Grounding IDs</title>
  <meta property="og:title" content="UNCOVERING GROUNDING IDS: HOW EXTERNAL CUES SHAPE MULTI-MODAL BINDING">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://vlm-cross-modal-reps.github.io">
  <!-- <meta name="description" content="Task representations in VLMs are consistent across modality (text, image) and specification (example, instruction)."> -->
  <!-- <meta name="keywords" content="vision-language models, multimodal models, task vectors, function vectors, in-context learning, interpretability"> -->

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RQ2WVP2RSN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-RQ2WVP2RSN');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/custom.css">
  <link rel="stylesheet" href="./static/css/fonts.css">
  <script src="./static/js/interactions.js"></script>
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding: 3rem 1.5rem 0.5rem 1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">UNCOVERING GROUNDING IDS:<br> HOW EXTERNAL CUES SHAPE MULTI-MODAL BINDING</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              <a href="https://graceluo.net" target="_blank">Grace Luo</a>
            </span>
            <span class="author-block">
              <a href="http://people.eecs.berkeley.edu/~trevor" target="_blank">Trevor Darrell</a>
            </span>
            <span class="author-block">
              <a href="https://www.amirbar.net" target="_blank">Amir Bar</a>
            </span> -->
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sharif University of Technology</span>
          </div>
          <div class="is-size-5 publication-authors">
            <br>
            <span class="author-block"><b>ICML 2026</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="http://arxiv.org/abs/2509.24072" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/sharif-ml-lab" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="padding-top: 1rem;">
  <div class="container is-max-desktop">
    <div class="is-centered has-text-centered">
      <p>
        <b>TLDR:</b> #TODO<br>
      </p>
      <br>
      Image or video here
      <!-- <div style="max-width: 720px;" class="max-width-content">
        <video id="teaser" muted playsinline onclick="playVideo('teaser')">
          <source src="assets/teaser.m4v" type="video/mp4">
        </video>
        <br>
        <button class="button is-white btn-teaser" onclick="playVideo('teaser')">
          <img style="margin-right: 5px;" src="assets/hand.svg" />
          Click to animate figure
        </button> -->
      </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-left">
          <p>
          Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of <em>Grounding IDs</em>, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as robust within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues.
          We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations.
          Taken together, our results identify Grounding IDs as a key symbolic mechanism explaining how external cues enhance multimodal binding, offering both interpretability and practical improvements in robustness.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Motivation</h2>
        <div class="max-width-content">
        


          <br>
          <div class="content has-text-left">
            <p>
              LVLMs have internal grounding capabilities and achieve strong performance on many benchmarks.
              However, they still struggle with structured reasoning, precise grounding, and avoiding hallucinations.
              Interestingly, very simple input modifications — like adding grids, lines, or annotations — can make these models much more reliable.
              Until now, it was not clear why such simple scaffolds help. We introduce Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities.
              For example, when an image is divided with visual symbols (#, @, $) and the prompt refers to them, the model internally creates identifiers that link each symbol to its corresponding region.
              These identifiers propagate through attention and embeddings, tightening the connection between image and text.
            </p>
          </div>

          <img src="assets/graphical_abstract.png" 
          alt="Graphical abstract of Grounding IDs" 
          class="max-width-results" />

          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Our Idea: Grounding IDs</h2>
        <div class="max-width-content">

          <br>
          <div class="content has-text-left">
            <p>
            We introduce Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. For example, when an image is divided with visual symbols (#, @, $) and the prompt refers to them, the model internally creates identifiers that link each symbol to its corresponding region. These identifiers propagate through attention and embeddings, tightening the connection between image and text.
            </p>
          </div>
          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Attention and Alignment: Evidence for Grounding IDs</h2>
        <div class="max-width-content">

          
          <br>
          <div class="content has-text-left">
            <p>
              To probe why external cues improve reasoning, we analyzed how LVLMs process partitioned inputs. Using a synthetic dataset of objects with added symbols and dividing lines, we compared baseline and structured conditions.\
              Attention maps revealed a clear difference: with cues, attention became strongly concentrated within partitions, both in visual–visual and text–visual interactions, indicating systematic binding. Embedding analysis provided complementary evidence.
              Measuring cosine similarity between matched visual and textual tokens showed that structured inputs consistently achieved higher cross-modal alignment, particularly in later layers of the model.
              Notably, the external symbols themselves reached even stronger alignment than the objects they marked. Together, these results show that simple cues induce Grounding IDs that propagate through attention and embeddings, effectively reducing the modality gap and reinforcing object–cue binding.
            </p>
          </div>
          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section>


<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Key Findings</h2>
        <div class="max-width-content">
          <!-- Video or image here -->
          <!-- <video class="max-width-results" id="instruction" muted playsinline onclick="playVideo('instruction')">
            <source src="assets/instruction.m4v" type="video/mp4">
          </video> -->
          
          <br>
          <div class="content has-text-left">
            <ul>
              <li>Grounding IDs consistently emerge when external cues are added.</li>
              <li>They reduce the modality gap by aligning visual and textual embeddings more tightly.</li>
              <li>They strengthen attention within partitions, improving cross-modal grounding.</li>
              <li>Both visual cues (lines, symbols) and textual cues (structured prompts) contribute, and their combination works best.</li>
            </ul>
          </div>

          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Causal Mediation and Activation Patching</h2>
        <div class="max-width-content">
          <!-- Video or image here -->
          <!-- <video class="max-width-results" id="instruction" muted playsinline onclick="playVideo('instruction')">
            <source src="assets/instruction.m4v" type="video/mp4">
          </video> -->
          
          <br>
          <div class="content has-text-left">
            <p>
              In addition to improving alignment, our work delves into causal mediation and activation patching to further refine the model’s internal processes. Causal mediation allows us to manipulate the external cues to see how they influence the model's reasoning, revealing how small changes in the input can lead to more accurate predictions. Furthermore, activation patching — a technique that involves adjusting the model’s internal activations — demonstrates that fine-tuning these patterns can lead to substantial performance improvements. These methods allow for more targeted control of the model’s behavior, directly improving its ability to reason and make accurate decisions.
            </p>
          </div>

          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="Practical Impact">Title here!!</h2>
        <div class="max-width-content">

          <br>
          <div class="content has-text-left">
            <p>
              Grounding IDs help LVLMs stay visually grounded during long text generation, reducing hallucinations. On benchmarks such as MS-COCO, this simple strategy outperforms or matches specialized hallucination-mitigation methods, with virtually no computational overhead. Importantly, the approach works for both open-source and closed-source models, making it broadly applicable.
              Grounding IDs highlight how simple external cues can activate symbolic mechanisms inside multimodal models. This provides both interpretability and practical gains in robustness. Future work may extend these insights to system-2 reasoning tasks, such as counting or spatial reasoning, and explore training methods that integrate cues directly.
            </p>
          </div>
          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- for more section :
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Title here!!</h2>
        <div class="max-width-content"> -->
          <!-- Video or image here -->
          <!-- <video class="max-width-results" id="instruction" muted playsinline onclick="playVideo('instruction')">
            <source src="assets/instruction.m4v" type="video/mp4">
          </video> -->
          
          <!-- <br>
          <div class="content has-text-left">
            <p>
            Text here
            </p>
          </div>
          <div class="finding-box">
            Text here!!</div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<hr>
<section class="section">

  <div class="container is-max-desktop content">
    <h2 class="title is-4">Relevant Readings</h2>
    <p>
      If you like this work, these other projects might also interest you.
    </p>
    <ul>
      <li>
        <p>
          #TODO:
          <a href="https://arxiv.org/abs/2310.15916" target="_blank">Hendel et al., 2023</a>; 
          <a href="https://functions.baulab.info" target="_blank">Todd et al., 2024</a>;
          <a href="https://arxiv.org/abs/2404.05729" target="_blank">Hojel et al., 2024</a>
        </p>
      </li>
    </ul>


    <h2 class="title is-4">Acknowledgements</h2>
    <p>
      #TODO
    </p>



    <h2 class="title is-4">BibTeX</h2>
    <pre><code>
    #TODO
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template is based on the
            <a href="https://nerfies.github.io">Nerfies</a> and
            <a href="https://readout-guidance.github.io">Readout Guidance</a> project pages.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
